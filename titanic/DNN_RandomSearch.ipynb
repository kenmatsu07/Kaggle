{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chainerを用いて深層学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "print \"sklearn_ver\",(sklearn.__version__)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from chainer import Chain, Variable, cuda, FunctionSet\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import datasets\n",
    "from chainer import optimizers\n",
    "from chainer import iterators\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer import report\n",
    "from chainer import serializers\n",
    "\n",
    "import chainer\n",
    "print \"chainer_ver\",(chainer.__version__)\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = ur\"dir_name\"\n",
    "output_dir = ur\"dir_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 入力ファイルを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(input_dir + \"flagon_merge.csv\", encoding=\"cp932\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データ型が定義されているファイルを読み込み\n",
    "type_list = ur\"/src/nttd/matsuura/data/type_list.csv\"\n",
    "df_type = pd.read_csv(type_list, encoding=\"cp932\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_rep = df_type.replace(\"VARCHAR2\", \"CHAR\")\n",
    "tmp12 = type_rep.copy()\n",
    "tmp01 = type_rep.copy()\n",
    "tmp12.names = map(lambda x: x + \"_12\", tmp12.names)\n",
    "tmp01.names = map(lambda x: x + \"_01\", tmp01.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_lis_concat = pd.concat([type_rep,tmp12,tmp01]).reset_index(drop =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 数値項目を抽出\n",
    "type_num = type_lis_concat.query('types == \"NUMBER\"')\n",
    "input_column = set(list(input_df.columns))\n",
    "num_column = set(list(type_num[\"names\"]))\n",
    "num_match_lis = list(input_column & num_column)\n",
    "print \"入力ファイルの項目数　：　NUMBER型の項目数　：\", len(input_df.columns), \"：\", len(num_match_lis)\n",
    "num_column_lis = input_df.loc[:,num_match_lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHAR型の項目を抽出\n",
    "type_char = type_lis_concat.query('types == \"CHAR\"')\n",
    "input_column = set(list(input_df.columns))\n",
    "char_column = set(list(type_char[\"names\"]))\n",
    "char_match_lis = list(input_column & char_column)\n",
    "print \"入力ファイルの項目数　：　CHAR型の項目数　：\", len(input_df.columns), \"：\", len(char_match_lis)\n",
    "char_column_lis = input_df.loc[:,char_match_lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# カテゴリデータは連続値と読み取られるため、ダミー変数に変換する\n",
    "# ダミー変数に変換するためデータ型を\"unicode\"にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_char_ex_dum = pd.get_dummies(char_column_lis, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_file = pd.concat([num_column_lis, df_char_ex_dum], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print merge_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_file_rename = merge_file.rename(columns = {u\"目的変数\":\"zandaka\"})\n",
    "print \"目的変数\", len(merge_file_rename.query('zandaka == 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 説明変数、目的変数への分割\n",
    "X = merge_file.drop(columns = {u\"目的変数\"},axis =1)\n",
    "y_zandaka = merge_file.loc[:,[u\"目的変数\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数値項目において訓練データの平均値/標準偏差から訓練データ、テストデータのz-scoreを算出し標準化する\n",
    "# StandardScalerは上記が実現不可のため個別実装\n",
    "def number_scaler(X_train, X_test):\n",
    "    \n",
    "    train_scaler = pd.DataFrame()\n",
    "    test_scaler = pd.DataFrame()\n",
    "    \n",
    "    train_num = X_train.loc[:, wk_num]\n",
    "    test_num = X_test.loc[:, wk_num]\n",
    "    \n",
    "    # 標準化後にカテゴリ項目をマージさせるために定義\n",
    "    train_char = X_train.loc[:, use_char_column]\n",
    "    test_char = X_test.loc[:, use_char_column]\n",
    "    \n",
    "    # 訓練データの平均値、標準偏差を取得\n",
    "    for i in train_num.columns:        \n",
    "        train_mean = train_num[i].mean()\n",
    "        train_std = train_num[i].std()\n",
    "        \n",
    "        # z-scoreを求める\n",
    "        train_z = pd.DataFrame({i:(train_num[i] - train_mean) / train_std})\n",
    "        test_z = pd.DataFrame({i:(test_num[i] - train_mean) / train_std})\n",
    "        \n",
    "        train_scaler = pd.concat([train_scaler, train_z], axis=1)\n",
    "        test_scaler = pd.concat([test_scaler, test_z], axis=1)\n",
    "    \n",
    "    # カテゴリ項目をマージ\n",
    "    train_scaler = pd.concat([train_scaler,train_char], axis=1)\n",
    "    test_scaler = pd.concat([test_scaler,test_char], axis=1)\n",
    "    \n",
    "    train_scaler = train_scaler.fillna(0)\n",
    "    test_scaler = test_scaler.fillna(0)\n",
    "    \n",
    "    return train_scaler, test_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_column = set(list(type_num[\"names\"]))\n",
    "\n",
    "X_column = list(X.columns)\n",
    "wk_num = list(set(X_column) & num_column)\n",
    "use_char_column = set(X_column) - set(wk_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 乱数シード\n",
    "random.seed(0)\n",
    "# 0:GPU、-1：CPU\n",
    "device_id = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 「訓練：テスト：検証　＝　7：2：1」に分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 訓練、テストデータへ分割　（7：3）\n",
    "(X_train, X_test, y_train_zan, y_test_zan) = train_test_split(X, y_zandaka, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 検証用データをテストデータから作成　（7：3）\n",
    "(X_test, X_valid, y_test_zan, y_valid) = train_test_split(X_test, y_test_zan, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 標準化\n",
    "X_train_scaler, X_test_scaler = number_scaler(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# テストデータの欠損値を削除した上で目的変数に欠損値が存在しない行を指定する\n",
    "X_test_scaler_kn = (X_test_scaler.replace([np.inf,-np.inf],np.nan)).dropna()\n",
    "noinf_ix = list(X_test_scaler_kn.index)\n",
    "y_test_zan_kn = y_test_zan.loc[noinf_ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chainer読込用に変換\n",
    "x_train_numpy = np.array(X_train_scaler, dtype = np.float32)\n",
    "x_test_numpy = np.array(X_test_scaler_kn, dtype = np.float32)\n",
    "y_train_zan_numpy = np.array(y_train_zan).astype(np.int32)\n",
    "y_test_zan_numpy = np.array(y_test_zan_kn).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chainer読込用に一次元配列に整形\n",
    "y_train_ex = y_train_zan_numpy.reshape(-1)\n",
    "y_test_ex = y_test_zan_numpy.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用デバイスがGPUの場合、cuda変換\n",
    "if device_id == 0:\n",
    "    # GPU用にcuda変換\n",
    "    x_train_gpu = cuda.to_gpu(x_train_numpy)\n",
    "    x_test_gpu = cuda.to_gpu(x_test_numpy)\n",
    "    y_train_gpu = cuda.to_gpu(y_train_ex)\n",
    "    y_test_gpu = cuda.to_gpu(y_test_ex)\n",
    "    \n",
    "    print x_train_gpu.dtype\n",
    "    print x_test_gpu.dtype\n",
    "    print y_train_gpu.dtype\n",
    "    print y_test_gpu.dtype\n",
    "    \n",
    "    # リスト化\n",
    "    train_zan = list(zip(x_train_gpu, y_train_gpu))\n",
    "    test_zan = list(zip(x_test_gpu, y_test_gpu))\n",
    "\n",
    "# CPUの場合、リスト化のみ行う\n",
    "else:\n",
    "    train_zan = list(zip(x_train_numpy, y_train_ex))\n",
    "    test_zan = list(zip(x_test_numpy, y_test_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 検証用データもテストデータと同様に加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 検証用データを標準化\n",
    "X_train_dum, X_valid_scaler = number_scaler(X_train, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 検証データの欠損値を削除した上で目的変数に欠損値が存在しない行を指定する\n",
    "X_valid_scaler_kn = (X_valid_scaler.replace([np.inf,-np.inf],np.nan)).dropna()\n",
    "noinf_valid = list(X_valid_scaler_kn.index)\n",
    "y_valid_zan_kn = y_valid.loc[noinf_valid, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chainer読込用に変換\n",
    "x_valid_numpy = np.array(X_valid_scaler_kn, dtype = np.float32)\n",
    "y_valid_zan_numpy = np.array(y_valid_zan_kn).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用デバイスがGPUの場合、cuda変換\n",
    "if device_id == 0:\n",
    "    #GPU用にcuda変換\n",
    "    x_valid_gpu = cuda.to_gpu(x_valid_numpy)\n",
    "    y_valid_gpu = cuda.to_gpu(y_valid_zan_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 深層学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NNモデル\n",
    "class MLP(Chain):\n",
    "\n",
    "    # 層定義\n",
    "    def __init__(self, n_units, activate, dropout_ratio, lenfeat):\n",
    "        super(MLP, self).__init__(\n",
    "            l1 = L.Linear(None, n_units),\n",
    "            l2 = L.Linear(None, 2),\n",
    "            \n",
    "            # 引数は適用する層の入力次元数と合わせる\n",
    "            bn1 = L.BatchNormalization(lenfeat),\n",
    "        )\n",
    "        self.activate = activate\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.train = True\n",
    "    \n",
    "    # 訓練/検証の切替\n",
    "    def set_train_state(self, train):\n",
    "        self.train = train\n",
    "    \n",
    "    # 順伝播\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # 活性化関数　（relu、sigmoid）\n",
    "        if self.activate == 'relu':\n",
    "            h1 = F.dropout(F.relu(self.l1(self.bn1(x, test = not self.train))), train=self.train ,ratio = self.dropout_ratio)\n",
    "        else: \n",
    "            h1 = F.dropout(F.sigmoid(self.l1(self.bn1(x, test = not self.train))), train=self.train ,ratio = self.dropout_ratio)\n",
    "        \n",
    "        return self.l2(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(L.Classifier):\n",
    "\n",
    "    def __init__(self, predictor, rowall, lossfun):\n",
    "        super(Classifier, self).__init__(predictor = predictor, lossfun=lossfun)\n",
    "        self.tn = self.fn = self.tp = self.fp = 0\n",
    "        self.call_cnt = 0\n",
    "        self.row_cnt = rowall\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        super(Classifier, self).__call__(x ,t)\n",
    "        y = self.predictor(x)\n",
    "        y_probs = F.softmax(y)        \n",
    "        predict = y_probs.data.argmax(axis=1)\n",
    "        #print \"predict:ans={}:{}\".format(len(predict), len(t.data))\n",
    "        \n",
    "        # confusion_matrix\n",
    "        tn,fn,tp,fp = self.calc_confusion_matrix(predict, t.data, len(t.data))\n",
    "        # auc\n",
    "        auc = self.calc_auc(y_probs.data[:,1], t.data)\n",
    "        report({'auc':auc, 'tn':tn, 'fn':fn, 'tp':tp, 'fp':fp}, self)\n",
    "        return self.loss\n",
    "\n",
    "    def calc_confusion_matrix(self, predict, label, datasize):\n",
    "        \"\"\"混合行列を計算する\n",
    "        \"\"\"\n",
    "        # 1epoch(データ件数/バッチサイズ)毎に\n",
    "        # __call__()が呼び出される回数を保持\n",
    "        self.call_cnt += 1\n",
    "        if self.row_cnt <= self.call_cnt * datasize:\n",
    "            self.tn = self.fn = self.tp = self.fp = 0\n",
    "            self.call_cnt = 0\n",
    "        for pred, ans in zip(predict, label):\n",
    "            if pred == ans == 0:\n",
    "                self.tn+=1\n",
    "            elif pred != ans == 0:\n",
    "                self.fn+=1\n",
    "            elif pred == ans == 1:\n",
    "                self.tp+=1\n",
    "            elif pred != ans == 1:\n",
    "                self.fp+=1\n",
    "            else:\n",
    "                print \"irregular-pattern! pred:{},ans:{}\".format(pred,ans)\n",
    "\n",
    "        return self.tn, self.fn, self.tp, self.fp\n",
    "\n",
    "    def calc_auc(self, prob, label):\n",
    "        \"\"\"aucを計算する\n",
    "        \"\"\"\n",
    "        n = len(prob)\n",
    "        a = 0.\n",
    "        score_prev = float('-inf')\n",
    "        fp = tp = 0\n",
    "        fp_prev = tp_prev = 0\n",
    "        # (prob,label)のセットでprobの値をキーに降順で並替え\n",
    "        tmp = sorted(zip(prob, label), key=lambda x:x[0], reverse=True)\n",
    "        tmp_u = zip(*tmp)\n",
    "        prob_desc = list(tmp_u[0])\n",
    "        label_desc = list(tmp_u[1])\n",
    "        for i in range(n):\n",
    "            if prob_desc[i] != score_prev:\n",
    "                # TP(FP)の増分がつくる長方形の面積を加算\n",
    "                a += self.trapezoid(fp, fp_prev, tp, tp_prev)\n",
    "                score_prev = prob_desc[i]\n",
    "                fp_prev = fp\n",
    "                tp_prev = tp\n",
    "\n",
    "            if label_desc[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        a += self.trapezoid(fp, fp_prev, tp, tp_prev)\n",
    "\n",
    "        # 最大面積で正規化\n",
    "        #print \"a:{},TP:{},FP:{}\".format(a,tp,fp)\n",
    "        if a == 0. or tp == 0 or fp == 0:\n",
    "            return 0.\n",
    "        else:\n",
    "            return a / (tp * fp)\n",
    "        \n",
    "    def trapezoid(self, x1, x2, y1, y2):\n",
    "        \"\"\"頂点を結んだ四角形の面積を求める\n",
    "        \"\"\"\n",
    "        width = abs(x1 - x2)\n",
    "        height = (y1 + y2) / 2.\n",
    "        return width * height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 固定パラメータ\n",
    "\n",
    "# バッチサイズ\n",
    "batchsize = 32\n",
    "# 学習回数\n",
    "n_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最適化対象のパラメータ\n",
    "# 他探索候補としてバッチサイズ、隠れ層の数\n",
    "\n",
    "# layer_range = range(2,6,1)\n",
    "\n",
    "# dropout率\n",
    "d_ratio_range = (0.1*x for x in xrange(1, 10))\n",
    "# 学習率\n",
    "lr_range = (0.01*x for x in xrange(1, 11))\n",
    "# WeightDecayの重み\n",
    "weight_range = (0.0001*x for x in xrange(1, 11))\n",
    "\n",
    "param_dist = {\"n_units\":[4,16,32,64,128,256], \n",
    "#               \"layer_num\":layer_range, \n",
    "              \"activate\":['relu', 'sigmoid'],\n",
    "              \"d_ratio\":d_ratio_range,\n",
    "              \"optimizer_name\":['SGD', 'MomentumSGD', 'Adam'],\n",
    "              \"lr\":lr_range,\n",
    "              \"wd\":weight_range\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 直積の組み合わせからパラメータを取得\n",
    "param_list = list(itertools.product(param_dist['n_units'], param_dist['activate'],param_dist['d_ratio'],\n",
    "                                    param_dist['optimizer_name'], param_dist['lr'], param_dist['wd']))\n",
    "print len(param_list)\n",
    "print type(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 簡易版ランダムサーチ\n",
    "def search_param(train_data, test_data, x_valid, y_valid):\n",
    "    \n",
    "    # 探索結果を詰める\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # TODO:実行回数は変数に変更\n",
    "    # ランダムサーチする回数は任意に設定\n",
    "    for i in xrange(1, 5):\n",
    "        \n",
    "        # 実行パラメータ、正解率、損失関数を詰める\n",
    "        work_lis = []\n",
    "\n",
    "        # パラメータの組み合わせからランダムに取得\n",
    "        sel_param = random.choice(param_list)\n",
    "        # ユニット数、活性化関数、dropout率、アルゴリズム、学習率、WeightDecayの重み　の順番\n",
    "        print \"実行パラメータ\", sel_param\n",
    "\n",
    "        # インスタンス生成\n",
    "        #clf = L.Classifier(MLP(sel_param[0], sel_param[1], sel_param[2]), lossfun = F.softmax_cross_entropy)\n",
    "        lenfeat = len(X.columns)\n",
    "        clf = Classifier(MLP(sel_param[0], sel_param[1], sel_param[2], lenfeat),\n",
    "                           len(train_data), lossfun=F.softmax_cross_entropy)\n",
    "\n",
    "        # 使用デバイス定義\n",
    "        if device_id >= 0:\n",
    "            chainer.cuda.get_device(device_id).use()\n",
    "            clf.to_gpu(device_id)\n",
    "        else:\n",
    "            clf.to_cpu()\n",
    "\n",
    "        # 選択されたアルゴリズムでoptimizerを設定\n",
    "        if sel_param[3] == 'SGD':\n",
    "            optimizer = optimizers.SGD(lr = sel_param[4])\n",
    "        elif sel_param[3] == 'MomentumSGD':\n",
    "            optimizer = optimizers.MomentumSGD(lr = sel_param[4])\n",
    "        else:\n",
    "            optimizer = optimizers.Adam()\n",
    "\n",
    "        optimizer.setup(clf)\n",
    "        optimizer.add_hook(chainer.optimizer.WeightDecay(sel_param[5]))\n",
    "\n",
    "        # Iterator\n",
    "        train_data_iter = iterators.SerialIterator(train_data, batchsize)\n",
    "        test_data_iter = iterators.SerialIterator(test_data, batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "        # trainerを用いて学習\n",
    "        updater = training.StandardUpdater(train_data_iter, optimizer, device = device_id)\n",
    "        trainer = training.Trainer(updater, (n_epoch, 'epoch'), out = 'result')\n",
    "\n",
    "        # 評価\n",
    "        trainer.extend(extensions.Evaluator(test_data_iter, clf, device = device_id))\n",
    "        # 学習結果を視覚化\n",
    "        trainer.extend(extensions.LogReport(log_name = \"search_param_zan_\" + str(i) + \".log\", trigger=(1, 'epoch')))\n",
    "        trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss','main/accuracy', 'validation/main/accuracy'], \n",
    "                                                 'epoch',  file_name='loss_zan_' + str(i) + '.png'))\n",
    "        trainer.extend(extensions.PlotReport(['main/auc', 'validation/main/auc'], x_key='epoch', file_name='auc_zan_' + str(i) + '.png'))\n",
    "        #trainer.extend(extensions.PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy', 'main/auc', 'validation/main/auc','elapsed_time']), trigger=(1,'epoch'))\n",
    "\n",
    "        # 学習の実行\n",
    "        trainer.run()\n",
    "\n",
    "        # テストデータの正解率/損失関数を取得\n",
    "        # ※　最上位だけ取得すると飛び値の可能性があるため、上位10位の平均スコアから求める\n",
    "        valid_data = trainer._extensions['PlotReport'].extension._data\n",
    "        loss_data = [data for i, data in valid_data['validation/main/loss']]\n",
    "        best10_loss = sorted(loss_data)[:10]\n",
    "        loss_avr = float(sum(best10_loss)) / 10\n",
    "\n",
    "        acc_data = [data for i, data in valid_data['validation/main/accuracy']]\n",
    "        best10_acc = sorted(acc_data)[:10]\n",
    "        acc_avr = float(sum(best10_acc)) / 10\n",
    "        \n",
    "        #print trainer._extensions.keys()\n",
    "        valid_data2 = trainer._extensions['PlotReport_1'].extension._data\n",
    "        auc_data = [data for i, data in valid_data2['validation/main/auc']]\n",
    "        best10_auc = sorted(auc_data)[:10]\n",
    "        auc_avr = float(sum(best10_auc)) / 10\n",
    "\n",
    "        # 実行パラメータ、正解率、損失関数を詰め込む\n",
    "        work_list = []\n",
    "        params = list(sel_param)\n",
    "\n",
    "        for param in params:\n",
    "            work_list.append(param)\n",
    "        \n",
    "        work_list.append(auc_avr)\n",
    "        work_list.append(loss_avr)\n",
    "        work_list.append(acc_avr)\n",
    "        tmp = pd.DataFrame(work_list).T\n",
    "        result = pd.concat([result, tmp], ignore_index=True)\n",
    "    \n",
    "    result.columns = [u\"ユニット数\", u\"活性化関数\", u\"dropout率\", u\"アルゴリズム\"\n",
    "                      , u\"学習率\", u\"WeightDecayの重み\", \"auc\", \"loss\", \"accuracy\"]\n",
    "    # ソートはauc：昇順\n",
    "    result_sort = result.sort_values(by='auc', ascending=False).reset_index(drop =True)\n",
    "        \n",
    "    return result_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zan_param = search_param(train_zan, test_zan, x_valid_numpy, y_valid_zan_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# パラメータ探索結果を出力\n",
    "display(zan_param)\n",
    "output = os.path.join(output_dir, \"search_param_zan.csv\")\n",
    "zan_param.to_csv(output, encoding=\"cp932\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最適パラメータで再度trainer実行\n",
    "lenfeat = len(X.columns)\n",
    "clf = Classifier(MLP(64,'relu', 0.2, lenfeat),\n",
    "                           len(train_zan), lossfun=F.softmax_cross_entropy)\n",
    "\n",
    "if device_id >= 0:\n",
    "    chainer.cuda.get_device(device_id).use()\n",
    "    clf.to_gpu(device_id)\n",
    "else:\n",
    "    clf.to_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optimizers.SGD(lr=0.07)\n",
    "optimizer.setup(clf)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(0.0006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO：最適パラメータ取得し再度モデル構築するまでを自動化\n",
    "# 一旦は動作確認優先のため、ランダムサーチで抽出した最適パラメータを手入力"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 最適パラメータで再度trainer実行\n",
    "lenfeat = len(X.columns)\n",
    "clf = Classifier(MLP(zan_param.iat[0, 0],zan_param.iat[0, 1], zan_param.iat[0, 2], lenfeat),\n",
    "                           len(train_zan), lossfun=F.softmax_cross_entropy)\n",
    "\n",
    "if device_id >= 0:\n",
    "    chainer.cuda.get_device(device_id).use()\n",
    "    clf.to_gpu(device_id)\n",
    "else:\n",
    "    clf.to_cpu()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# optimizer\n",
    "optimizer = optimizers.SGD(lr=zan_param.iat[0, 4])\n",
    "optimizer.setup(clf)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(zan_param.iat[0, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterator\n",
    "train_data_iter = iterators.SerialIterator(train_zan, batchsize)\n",
    "test_data_iter = iterators.SerialIterator(test_zan, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO：学習が最大化したら学習を辞め、optimizer出力する実装を追加\n",
    "#        (現状では過学習モデルが過学習されたまま出力されてしまうため、検証結果が悪くなる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainerを用いて学習\n",
    "updater = training.StandardUpdater(train_data_iter, optimizer, device = device_id)\n",
    "trainer = training.Trainer(updater, (n_epoch, 'epoch'), out = 'result')\n",
    "\n",
    "# 評価\n",
    "trainer.extend(extensions.Evaluator(test_data_iter, clf, device = device_id))\n",
    "# 学習結果を視覚化\n",
    "trainer.extend(extensions.LogReport(log_name = \"best_param_zan.log\", trigger=(1, 'epoch')))\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss','main/accuracy', 'validation/main/accuracy'], \n",
    "                                                 'epoch',  file_name='loss_zan.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習の実行\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logファイルの読み込み\n",
    "with open('result/best_param_zan.log') as f:\n",
    "    logs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習経過をプロット　（損失関数）\n",
    "result_train, result_test = [], []\n",
    "for log in logs:\n",
    "    result_train.append(log['main/loss'])\n",
    "    result_test.append(log['validation/main/loss'])\n",
    "\n",
    "# グラフ描画\n",
    "plt.plot(result_train, label='train')\n",
    "plt.plot(result_test, label='test')\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Loss Progress\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習経過をプロット　（正解率）\n",
    "result_train2, result_test2 = [], []\n",
    "for log in logs:\n",
    "    result_train2.append(log['main/accuracy'])\n",
    "    result_test2.append(log['validation/main/accuracy'])\n",
    "\n",
    "# グラフ描画\n",
    "plt.plot(result_train2, label='train')\n",
    "plt.plot(result_test2, label='test')\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Accuracy Progress\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "#cpu_model = clf.to_cpu()\n",
    "serializers.save_npz(\"./model/chainer_zan.model\",clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#モデルの読み込み\n",
    "serializers.load_npz('./model/chainer_zan.model', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "of = open('chainer_zan.pkl', 'wb')\n",
    "pickle.dump(clf, of)\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデル検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 検証用モデル\n",
    "evaluator = clf.copy()\n",
    "# 検証モードに切替\n",
    "evaluator.predictor.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print clf.predictor.train\n",
    "print evaluator.predictor.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 検証モードで前進計算\n",
    "y_pre = evaluator.predictor(x_valid_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print y_pre.data\n",
    "print len(y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ソフトマックス算出して分類確率を出力\n",
    "y_prob = F.softmax(y_pre)\n",
    "y_prob.data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pre2 = y_prob.data.argmax(axis=1)\n",
    "print y_pre2\n",
    "print len(y_pre2)\n",
    "print len(np.where(y_pre2 == 0)[0])\n",
    "print len(np.where(y_pre2 == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aucを出力\n",
    "precision, recall, threshold = precision_recall_curve(y_valid_zan_kn, y_pre2)\n",
    "print \"precision \",precision\n",
    "print \"recall\",recall\n",
    "print \"threshold\",threshold\n",
    "\n",
    "print \"\"\n",
    "print classification_report(y_valid_zan_kn, y_pre2)\n",
    "print \"\"\n",
    "\n",
    "print \"confusion_matrix:\"\n",
    "print confusion_matrix(y_valid_zan_kn, y_pre2)\n",
    "print \"\"\n",
    "\n",
    "fpr,tpr,thresholds = roc_curve(y_valid_zan_kn, y_prob.data[:,1])\n",
    "print \"auc\", round(auc(fpr,tpr),4)\n",
    "\n",
    "# ROC曲線を出力\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"False Positve Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
